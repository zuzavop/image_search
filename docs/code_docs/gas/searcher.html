<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>gas.searcher API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>gas.searcher</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import clip
import numpy as np
import torch


class Searcher:
    &#34;&#34;&#34;
    The Searcher class is responsible for searching through a given set of images using text or image queries.
    It utilizes the CLIP (Contrastive Language-Image Pre-training) model to encode text queries to n-dimensional space.

    Attributes:
        clip_data (list): A list of feature vectors representing the images.
        combination (bool): A boolean flag indicating whether to combine the scores of the current and previous search
            queries. If True, the last search scores are added to the current scores.
            If False, only the current scores are used.
        logger (Logger): A Logger instance for logging search queries and results.
        showing (int): The number of top search results which are display.
        last_search (dict): A dictionary to store the vectors of the last text search for each session.
        device: A string indicating whether to use CPU or GPU for running the CLIP model.
        model: The pre-trained CLIP model.
    &#34;&#34;&#34;

    def __init__(self, clip_data, combination, logger, showing):
        &#34;&#34;&#34;
        Args:
            clip_data (list): A list of feature vectors representing the images.
            combination (bool): A boolean flag indicating whether to combine the scores of the current and previous
                search queries.
            logger (Logger): A Logger instance for logging search queries and results.
            showing (int): The number of top search results which are display.
        &#34;&#34;&#34;
        self.clip_data = clip_data
        self.combination = combination
        self.last_search = {}  # vectors of last text search
        self.logger = logger
        self.showing = showing
        # clip
        self.device = &#34;cuda&#34; if torch.cuda.is_available() else &#34;cpu&#34;
        self.model, preprocess = clip.load(&#34;ViT-B/32&#34;, device=self.device)

    def result_score(self, features):
        &#34;&#34;&#34;
        Calculate the similarity distance of the query feature vector to the CLIP data
        (normalize feature vectors of images).

        Args:
            features (numpy.ndarray): A 2D array representing the normalize feature vector of the query.

        Returns:
            numpy.ndarray: A 1D array representing the similarity distance (from 0 to 2) of each image to the query.
        &#34;&#34;&#34;
        return np.concatenate([1 - (torch.cat(self.clip_data) @ features)], axis=None)

    def text_search(self, query, session, found, activity):
        &#34;&#34;&#34;
        Text search using CLIP data.

        Args:
            query (str): The text query.
            session (str): The unique session ID of the user. (used for logging)
            found (int): The index of the currently searching image. (used for logging)
            activity (str): The activity from the user. (used for logging)

        Returns:
            list: A list of indices representing the top search results.
        &#34;&#34;&#34;
        # get normalize features of text query
        with torch.no_grad():
            text_features = self.model.encode_text(clip.tokenize([query]).to(self.device))
        text_features /= np.linalg.norm(text_features)

        # get distance of vectors
        scores = self.result_score(text_features.T)

        new_scores = list(np.argsort((scores + self.last_search[session]) if self.combination else scores))
        # save score for next search
        if self.combination:
            self.last_search[session] = scores

        self.logger.log_text_query(query, new_scores, found, session, activity)

        return new_scores[:self.showing]

    def image_search(self, image_query, found, session):
        &#34;&#34;&#34;
        Image search using CLIP data.

        Args:
            image_query (int): The index of the image query.
            found (int): The index of the currently searching image. (used for logging)
            session (str): The unique session ID of the user. (used for logging)

        Returns:
            list: A list of indices representing the top search results.
        &#34;&#34;&#34;
        # get features of image query
        image_query_index = int(image_query)
        image_query_features = np.transpose(self.clip_data[image_query_index])

        scores = list(np.argsort(self.result_score(image_query_features)))

        self.logger.log_image_query(image_query, scores, found, session)

        return scores[:self.showing]

    def reset_last(self, session):
        &#34;&#34;&#34;
        Reset scores of last search for user of given session.

        Args:
            session (str): The unique session ID of the user.
        &#34;&#34;&#34;
        self.last_search[session] = np.zeros(len(self.clip_data))</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="gas.searcher.Searcher"><code class="flex name class">
<span>class <span class="ident">Searcher</span></span>
<span>(</span><span>clip_data, combination, logger, showing)</span>
</code></dt>
<dd>
<div class="desc"><p>The Searcher class is responsible for searching through a given set of images using text or image queries.
It utilizes the CLIP (Contrastive Language-Image Pre-training) model to encode text queries to n-dimensional space.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>clip_data</code></strong> :&ensp;<code>list</code></dt>
<dd>A list of feature vectors representing the images.</dd>
<dt><strong><code>combination</code></strong> :&ensp;<code>bool</code></dt>
<dd>A boolean flag indicating whether to combine the scores of the current and previous search
queries. If True, the last search scores are added to the current scores.
If False, only the current scores are used.</dd>
<dt><strong><code>logger</code></strong> :&ensp;<code>Logger</code></dt>
<dd>A Logger instance for logging search queries and results.</dd>
<dt><strong><code>showing</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of top search results which are display.</dd>
<dt><strong><code>last_search</code></strong> :&ensp;<code>dict</code></dt>
<dd>A dictionary to store the vectors of the last text search for each session.</dd>
<dt><strong><code>device</code></strong></dt>
<dd>A string indicating whether to use CPU or GPU for running the CLIP model.</dd>
<dt><strong><code>model</code></strong></dt>
<dd>The pre-trained CLIP model.</dd>
</dl>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>clip_data</code></strong> :&ensp;<code>list</code></dt>
<dd>A list of feature vectors representing the images.</dd>
<dt><strong><code>combination</code></strong> :&ensp;<code>bool</code></dt>
<dd>A boolean flag indicating whether to combine the scores of the current and previous
search queries.</dd>
<dt><strong><code>logger</code></strong> :&ensp;<code>Logger</code></dt>
<dd>A Logger instance for logging search queries and results.</dd>
<dt><strong><code>showing</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of top search results which are display.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Searcher:
    &#34;&#34;&#34;
    The Searcher class is responsible for searching through a given set of images using text or image queries.
    It utilizes the CLIP (Contrastive Language-Image Pre-training) model to encode text queries to n-dimensional space.

    Attributes:
        clip_data (list): A list of feature vectors representing the images.
        combination (bool): A boolean flag indicating whether to combine the scores of the current and previous search
            queries. If True, the last search scores are added to the current scores.
            If False, only the current scores are used.
        logger (Logger): A Logger instance for logging search queries and results.
        showing (int): The number of top search results which are display.
        last_search (dict): A dictionary to store the vectors of the last text search for each session.
        device: A string indicating whether to use CPU or GPU for running the CLIP model.
        model: The pre-trained CLIP model.
    &#34;&#34;&#34;

    def __init__(self, clip_data, combination, logger, showing):
        &#34;&#34;&#34;
        Args:
            clip_data (list): A list of feature vectors representing the images.
            combination (bool): A boolean flag indicating whether to combine the scores of the current and previous
                search queries.
            logger (Logger): A Logger instance for logging search queries and results.
            showing (int): The number of top search results which are display.
        &#34;&#34;&#34;
        self.clip_data = clip_data
        self.combination = combination
        self.last_search = {}  # vectors of last text search
        self.logger = logger
        self.showing = showing
        # clip
        self.device = &#34;cuda&#34; if torch.cuda.is_available() else &#34;cpu&#34;
        self.model, preprocess = clip.load(&#34;ViT-B/32&#34;, device=self.device)

    def result_score(self, features):
        &#34;&#34;&#34;
        Calculate the similarity distance of the query feature vector to the CLIP data
        (normalize feature vectors of images).

        Args:
            features (numpy.ndarray): A 2D array representing the normalize feature vector of the query.

        Returns:
            numpy.ndarray: A 1D array representing the similarity distance (from 0 to 2) of each image to the query.
        &#34;&#34;&#34;
        return np.concatenate([1 - (torch.cat(self.clip_data) @ features)], axis=None)

    def text_search(self, query, session, found, activity):
        &#34;&#34;&#34;
        Text search using CLIP data.

        Args:
            query (str): The text query.
            session (str): The unique session ID of the user. (used for logging)
            found (int): The index of the currently searching image. (used for logging)
            activity (str): The activity from the user. (used for logging)

        Returns:
            list: A list of indices representing the top search results.
        &#34;&#34;&#34;
        # get normalize features of text query
        with torch.no_grad():
            text_features = self.model.encode_text(clip.tokenize([query]).to(self.device))
        text_features /= np.linalg.norm(text_features)

        # get distance of vectors
        scores = self.result_score(text_features.T)

        new_scores = list(np.argsort((scores + self.last_search[session]) if self.combination else scores))
        # save score for next search
        if self.combination:
            self.last_search[session] = scores

        self.logger.log_text_query(query, new_scores, found, session, activity)

        return new_scores[:self.showing]

    def image_search(self, image_query, found, session):
        &#34;&#34;&#34;
        Image search using CLIP data.

        Args:
            image_query (int): The index of the image query.
            found (int): The index of the currently searching image. (used for logging)
            session (str): The unique session ID of the user. (used for logging)

        Returns:
            list: A list of indices representing the top search results.
        &#34;&#34;&#34;
        # get features of image query
        image_query_index = int(image_query)
        image_query_features = np.transpose(self.clip_data[image_query_index])

        scores = list(np.argsort(self.result_score(image_query_features)))

        self.logger.log_image_query(image_query, scores, found, session)

        return scores[:self.showing]

    def reset_last(self, session):
        &#34;&#34;&#34;
        Reset scores of last search for user of given session.

        Args:
            session (str): The unique session ID of the user.
        &#34;&#34;&#34;
        self.last_search[session] = np.zeros(len(self.clip_data))</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="gas.searcher.Searcher.image_search"><code class="name flex">
<span>def <span class="ident">image_search</span></span>(<span>self, image_query, found, session)</span>
</code></dt>
<dd>
<div class="desc"><p>Image search using CLIP data.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>image_query</code></strong> :&ensp;<code>int</code></dt>
<dd>The index of the image query.</dd>
<dt><strong><code>found</code></strong> :&ensp;<code>int</code></dt>
<dd>The index of the currently searching image. (used for logging)</dd>
<dt><strong><code>session</code></strong> :&ensp;<code>str</code></dt>
<dd>The unique session ID of the user. (used for logging)</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>list</code></dt>
<dd>A list of indices representing the top search results.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def image_search(self, image_query, found, session):
    &#34;&#34;&#34;
    Image search using CLIP data.

    Args:
        image_query (int): The index of the image query.
        found (int): The index of the currently searching image. (used for logging)
        session (str): The unique session ID of the user. (used for logging)

    Returns:
        list: A list of indices representing the top search results.
    &#34;&#34;&#34;
    # get features of image query
    image_query_index = int(image_query)
    image_query_features = np.transpose(self.clip_data[image_query_index])

    scores = list(np.argsort(self.result_score(image_query_features)))

    self.logger.log_image_query(image_query, scores, found, session)

    return scores[:self.showing]</code></pre>
</details>
</dd>
<dt id="gas.searcher.Searcher.reset_last"><code class="name flex">
<span>def <span class="ident">reset_last</span></span>(<span>self, session)</span>
</code></dt>
<dd>
<div class="desc"><p>Reset scores of last search for user of given session.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>session</code></strong> :&ensp;<code>str</code></dt>
<dd>The unique session ID of the user.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reset_last(self, session):
    &#34;&#34;&#34;
    Reset scores of last search for user of given session.

    Args:
        session (str): The unique session ID of the user.
    &#34;&#34;&#34;
    self.last_search[session] = np.zeros(len(self.clip_data))</code></pre>
</details>
</dd>
<dt id="gas.searcher.Searcher.result_score"><code class="name flex">
<span>def <span class="ident">result_score</span></span>(<span>self, features)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate the similarity distance of the query feature vector to the CLIP data
(normalize feature vectors of images).</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>features</code></strong> :&ensp;<code>numpy.ndarray</code></dt>
<dd>A 2D array representing the normalize feature vector of the query.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>numpy.ndarray</code></dt>
<dd>A 1D array representing the similarity distance (from 0 to 2) of each image to the query.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def result_score(self, features):
    &#34;&#34;&#34;
    Calculate the similarity distance of the query feature vector to the CLIP data
    (normalize feature vectors of images).

    Args:
        features (numpy.ndarray): A 2D array representing the normalize feature vector of the query.

    Returns:
        numpy.ndarray: A 1D array representing the similarity distance (from 0 to 2) of each image to the query.
    &#34;&#34;&#34;
    return np.concatenate([1 - (torch.cat(self.clip_data) @ features)], axis=None)</code></pre>
</details>
</dd>
<dt id="gas.searcher.Searcher.text_search"><code class="name flex">
<span>def <span class="ident">text_search</span></span>(<span>self, query, session, found, activity)</span>
</code></dt>
<dd>
<div class="desc"><p>Text search using CLIP data.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>query</code></strong> :&ensp;<code>str</code></dt>
<dd>The text query.</dd>
<dt><strong><code>session</code></strong> :&ensp;<code>str</code></dt>
<dd>The unique session ID of the user. (used for logging)</dd>
<dt><strong><code>found</code></strong> :&ensp;<code>int</code></dt>
<dd>The index of the currently searching image. (used for logging)</dd>
<dt><strong><code>activity</code></strong> :&ensp;<code>str</code></dt>
<dd>The activity from the user. (used for logging)</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>list</code></dt>
<dd>A list of indices representing the top search results.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def text_search(self, query, session, found, activity):
    &#34;&#34;&#34;
    Text search using CLIP data.

    Args:
        query (str): The text query.
        session (str): The unique session ID of the user. (used for logging)
        found (int): The index of the currently searching image. (used for logging)
        activity (str): The activity from the user. (used for logging)

    Returns:
        list: A list of indices representing the top search results.
    &#34;&#34;&#34;
    # get normalize features of text query
    with torch.no_grad():
        text_features = self.model.encode_text(clip.tokenize([query]).to(self.device))
    text_features /= np.linalg.norm(text_features)

    # get distance of vectors
    scores = self.result_score(text_features.T)

    new_scores = list(np.argsort((scores + self.last_search[session]) if self.combination else scores))
    # save score for next search
    if self.combination:
        self.last_search[session] = scores

    self.logger.log_text_query(query, new_scores, found, session, activity)

    return new_scores[:self.showing]</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="gas" href="index.html">gas</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="gas.searcher.Searcher" href="#gas.searcher.Searcher">Searcher</a></code></h4>
<ul class="">
<li><code><a title="gas.searcher.Searcher.image_search" href="#gas.searcher.Searcher.image_search">image_search</a></code></li>
<li><code><a title="gas.searcher.Searcher.reset_last" href="#gas.searcher.Searcher.reset_last">reset_last</a></code></li>
<li><code><a title="gas.searcher.Searcher.result_score" href="#gas.searcher.Searcher.result_score">result_score</a></code></li>
<li><code><a title="gas.searcher.Searcher.text_search" href="#gas.searcher.Searcher.text_search">text_search</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>